# ğŸ“˜ CenSQL â€“ Text-to-SQL for Indian Census Data

This project implements a **Text-to-SQL system** for querying **Indian Census Data** using **LLaMA-3-SQLCoder + LoRA** with a **FastAPI backend**, **Supabase PostgreSQL database**, and **Next.js frontend**.

---

## ğŸ“‚ File Structure

``` text
censql/
â”‚
â”œâ”€â”€ data/                   # All dataset-related files
â”‚   â”œâ”€â”€ raw/                # Raw CSV / Excel / PDF downloads
â”‚   â”œâ”€â”€ clean/              # Cleaned datasets (CSV)
â”‚   â”œâ”€â”€ schema/             # Schema docs + CREATE TABLE SQL
â”‚   â””â”€â”€ census_demo.db      # Local SQLite (for quick testing)
â”‚
â”œâ”€â”€ src/                    # Core Python backend code
â”‚   â”œâ”€â”€ data_ingest.py      # Data ingestion & cleaning classes
â”‚   â”œâ”€â”€ sql_saver.py        # Save CSVs â†’ SQL DB + dumps
â”‚   â”œâ”€â”€ model_client.py     # Wrapper to call local/cloud model
â”‚   â”œâ”€â”€ pipeline.py         # QueryRouter (templates + model path)
â”‚   â”œâ”€â”€ backend_fastapi.py  # FastAPI server (API routes)
â”‚   â””â”€â”€ local_infer.py      # Local inference wrapper for SQLCoder
â”‚
â”œâ”€â”€ frontend/               # Next.js frontend
â”‚   â”œâ”€â”€ app/                # Next.js app directory
â”‚   â””â”€â”€ components/         # UI components
â”‚
â”œâ”€â”€ notebooks/              # Jupyter/Colab notebooks for testing
â”‚
â”œâ”€â”€ logs/                   # Query logs + metrics
â”‚   â””â”€â”€ query_log.csv
â”‚
â”œâ”€â”€ environment.yml         # Conda environment definition
â”œâ”€â”€ requirements.txt        # Pip dependencies
â””â”€â”€ README.md               # Project overview (this file)
```

---

## âš™ï¸ Coding Conventions

### General

* Use **OOP principles**: each module should define a **class** (e.g., `DataIngestor`, `SQLSaver`, `ModelClient`, `QueryRouter`).
* Follow **snake_case** for file & function names.
* Follow **PascalCase** for class names.
* Add **docstrings** to every class and function.
* Keep **logging** (not print statements) for debugging.

### Data

* All raw files â†’ `data/raw/`
* All cleaned files â†’ `data/clean/`
* Schema docs (`schema.md`, `schema.sql`) â†’ `data/schema/`
* Always export cleaned data as `.csv` and `.sql` dump for reproducibility.

### Backend

* API endpoints must be under `/api/` (e.g., `/api/query`).
* `run_pipeline()` should always return:

  ```json
  {
    "path": "template" | "model",
    "sql": "...",
    "result": [...],
    "latency_ms": 123
  }
  ```

### Frontend

* Use **Next.js (TypeScript)**.
* All API calls â†’ `/api/query` endpoint.
* Show **Query, SQL, Result, Path, Latency**.

---

## ğŸ‘¥ Team Responsibilities

### **Nandhini & Gopikha** â€“ Data & Database

* Collect datasets (CSV, Excel, PDF) from portals.
* Write ingestion + cleaning scripts (`data_ingest.py`).
* Normalize columns (snake_case).
* Load into Supabase Postgres via `sql_saver.py`.
* Maintain schema docs (`schema/schema.md`).

### **Sourish** â€“ Model & Backend Integration

* Implement base model client (`local_infer.py` + `model_client.py`).
* Test LLaMA-3-SQLCoder inference without LoRA.
* Integrate with FastAPI backend (`backend_fastapi.py`).
* Prepare `ModelClient` for cloud access (LoRA later).

### **Maharajan** â€“ Frontend

* Build Next.js UI (`frontend/`).
* Input: Natural language query.
* Output: SQL, result table, path (template/model), latency.
* Style UI (free choice of design).
* Deploy frontend (Vercel).

### **All Members**

* Follow file structure & conventions.
* Push work to respective folders.
* Update `README.md` with any new scripts.
* Work on PPT slides: Data, Model, Backend, Frontend, Evaluation.

---

## ğŸš€ Workflow

1. **Data** â†’ Collect, clean, save to Supabase.
2. **Backend** â†’ FastAPI pipeline (rule-based + model).
3. **Frontend** â†’ Next.js app calling backend API.
4. **Integration** â†’ End-to-end demo (NL â†’ SQL â†’ Census DB â†’ Result).
5. **Evaluation** â†’ Log metrics (Exact Match, Execution Accuracy, Latency).
